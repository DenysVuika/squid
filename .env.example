# =============================================================================
# squid Configuration Example
# =============================================================================
# Copy this file to .env and update with your settings
# Choose ONE of the configurations below based on your LLM provider

# -----------------------------------------------------------------------------
# Logging Configuration
# -----------------------------------------------------------------------------
LOG_LEVEL=error

# -----------------------------------------------------------------------------
# Context Window Configuration
# -----------------------------------------------------------------------------
# Maximum context window size in tokens for your model
# This enables accurate token usage tracking and prevents exceeding model limits
# Default: 8192 tokens if not specified
CONTEXT_WINDOW=32768

# =============================================================================
# OPTION 1: LM Studio (Recommended for Local Development)
# =============================================================================
# 1. Download LM Studio from https://lmstudio.ai/
# 2. Download model: lmstudio-community/Qwen2.5-Coder-7B-Instruct-MLX-4bit
# 3. Load the model in LM Studio
# 4. Start local server (↔️ icon → "Start Server")
# 5. Use these settings:

API_URL=http://127.0.0.1:1234/v1
API_MODEL=local-model
API_KEY=not-needed
CONTEXT_WINDOW=32768  # Qwen2.5-Coder default

# -----------------------------------------------------------------------------
# OPTION 2: Ollama (Lightweight CLI Alternative)
# -----------------------------------------------------------------------------
# 1. Install: brew install ollama (macOS) or see https://ollama.com/
# 2. Start service: ollama serve
# 3. Pull model: ollama pull qwen2.5-coder
# 4. Use these settings:

# API_URL=http://localhost:11434/v1
# API_MODEL=qwen2.5-coder
# API_KEY=not-needed
# CONTEXT_WINDOW=32768  # Qwen2.5-Coder: 32K tokens

# Alternative Ollama models:
# API_MODEL=codellama          # Code generation focused (16K context)
# API_MODEL=deepseek-coder     # Code understanding (16K context)
# API_MODEL=llama3.1           # General purpose (128K context)

# -----------------------------------------------------------------------------
# OPTION 3: OpenAI API (Cloud Service)
# -----------------------------------------------------------------------------
# 1. Get API key from https://platform.openai.com/api-keys
# 2. Add credits to your account
# 3. Use these settings:

# API_URL=https://api.openai.com/v1
# API_MODEL=gpt-4
# API_KEY=sk-your-actual-api-key-here
# CONTEXT_WINDOW=128000  # GPT-4: 128K tokens

# Alternative OpenAI models:
# API_MODEL=gpt-4-turbo        # Faster, cheaper GPT-4 (128K context)
# API_MODEL=gpt-3.5-turbo      # Fastest, cheapest (16K context)

# -----------------------------------------------------------------------------
# OPTION 4: Mistral API (Cloud Service)
# -----------------------------------------------------------------------------
# 1. Get API key from https://console.mistral.ai/
# 2. Choose a model (devstral-2512, mistral-large-latest, etc.)
# 3. Use these settings:

# API_URL=https://api.mistral.ai/v1
# API_MODEL=devstral-2512
# API_KEY=your-mistral-api-key-here
# CONTEXT_WINDOW=131072  # Mistral Large: 128K tokens

# Alternative Mistral models:
# API_MODEL=mistral-large-latest    # Most capable (128K context)
# API_MODEL=mistral-small-latest    # Fast and efficient (32K context)
# API_MODEL=codestral-latest        # Code-specialized (32K context)

# -----------------------------------------------------------------------------
# OPTION 5: Other OpenAI-Compatible Services
# -----------------------------------------------------------------------------

# OpenRouter (https://openrouter.ai/)
# API_URL=https://openrouter.ai/api/v1
# API_MODEL=anthropic/claude-3-sonnet
# API_KEY=your-openrouter-key

# Together AI (https://together.ai/)
# API_URL=https://api.together.xyz/v1
# API_MODEL=codellama/CodeLlama-34b-Instruct-hf
# API_KEY=your-together-ai-key

# Custom OpenAI-compatible endpoint
# API_URL=http://your-custom-endpoint:port/v1
# API_MODEL=your-model-name
# API_KEY=your-api-key-if-needed

# =============================================================================
# Common Context Window Sizes
# =============================================================================
# Qwen2.5-Coder-7B:  32768   (32K tokens)
# GPT-4/GPT-4o:      128000  (128K tokens)
# GPT-3.5-turbo:     16385   (16K tokens)
# Claude 3 Opus:     200000  (200K tokens)
# Claude 3.5 Sonnet: 200000  (200K tokens)
# Llama 3.1-8B:      131072  (128K tokens)
# Mistral Large:     131072  (128K tokens)
# DeepSeek Coder:    16384   (16K tokens)
# CodeLlama:         16384   (16K tokens)
#
# How to find your model's context window:
# 1. Check model documentation on Hugging Face
# 2. Look in the model card or config.json
# 3. Check your LLM provider's documentation
# 4. For LM Studio: Look at model details in the UI

# =============================================================================
# Notes
# =============================================================================
# - Only ONE configuration should be active (uncomment the one you want)
# - LM Studio and Ollama don't require API keys (use "not-needed")
# - For best code-related results, use qwen2.5-coder model
# - Streaming is supported with all providers (use -s or --stream flag)
# - Set CONTEXT_WINDOW to match your model for accurate usage tracking
